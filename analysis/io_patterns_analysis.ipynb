{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O Patterns and Bottlenecks in Deep Learning Workloads\n",
    "\n",
    "**Author:** Pablo Alessandro Santos Hugen  \n",
    "**Institution:** Institute of Informatics -- UFRGS  \n",
    "**Course:** Computer Systems Performance Analysis 2025/2\n",
    "\n",
    "---\n",
    "\n",
    "This notebook serves as the **central controller** for the entire experimental workflow:\n",
    "1. Environment setup and configuration\n",
    "2. Loading the experimental design\n",
    "3. Running DLIO benchmarks\n",
    "4. Collecting and analyzing results\n",
    "\n",
    "**Prerequisites:**\n",
    "- Allocate an interactive node: `salloc --partition=<partition> --nodes=1 --ntasks=8 --time=4:00:00`\n",
    "- Launch Jupyter from the allocated node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Context\n",
    "\n",
    "Recent years have seen growing interest in optimizations for Machine Learning and Deep Learning training and inference methods. These techniques are now used across various fields, including Large Language Models (LLMs), image recognition and classification, and many other applications.\n",
    "\n",
    "Large models often require substantial HPC infrastructures to process the enormous amounts of training data involved. In this context, **the performance of the storage and I/O subsystem is critical**.\n",
    "\n",
    "#### Traditional HPC vs. ML Workloads\n",
    "\n",
    "| Aspect | Traditional HPC | ML Workloads |\n",
    "|--------|-----------------|---------------|\n",
    "| Access Pattern | Large, sequential reads/writes | Small, random reads across numerous files |\n",
    "| Typical Use Case | Simulations with periodic checkpoints | Iterative training over dataset epochs |\n",
    "| I/O Characteristics | Predictable, burst-oriented | Continuous, irregular access patterns |\n",
    "\n",
    "### 1.2 The I/O Bottleneck Problem\n",
    "\n",
    "At large-scale distributed DL workloads:\n",
    "- **I/O can take roughly 85% of the training time** (Mohan et al., 2021)\n",
    "- Training is often one of the most expensive parts of the ML pipeline (Chowdhury et al., 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Objectives\n",
    "\n",
    "### 2.1 General Objective\n",
    "\n",
    "Understand **patterns in I/O operations and possible bottlenecks** in common Machine Learning workloads.\n",
    "\n",
    "### 2.2 Specific Objectives\n",
    "\n",
    "1. **Disk Throughput:** Understand how disk throughput varies during training between epochs, checkpoints, and when the number of training processes varies.\n",
    "\n",
    "2. **GPU Usage:** Analyze how GPU usage (%) behaves in those scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Setup\n",
    "\n",
    "### 3.1 Configuration\n",
    "\n",
    "Configure the environment variables for your cluster below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#==============================================================================\n# CONFIGURATION - Modify these for your cluster\n#==============================================================================\n\n# Environment modules to load (same as bench.slurm)\nMODULES = \"arch_gpu_sc/current openmpi/4.1.6.15.1\"\n\n# Paths\nBASE_DIR = Path(\"..\").resolve()\nCONFIG_DIR = BASE_DIR / \"config\"\nRESULTS_DIR = BASE_DIR / \"results\"\nEXPERIMENT_FILE = Path(\"experimental_design.csv\")\n\n# Create scratch directory for benchmark data\nSCRATCH_DIR = BASE_DIR / f\"dlio_data_{os.getpid()}\"\n\nprint(f\"Modules: {MODULES}\")\nprint(f\"Base directory: {BASE_DIR}\")\nprint(f\"Config directory: {CONFIG_DIR}\")\nprint(f\"Results directory: {RESULTS_DIR}\")\nprint(f\"Scratch directory: {SCRATCH_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Helper Functions\n",
    "\n",
    "Functions for running shell commands and managing the benchmark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_command(cmd: str, cwd: Path = None, verbose: bool = True, load_modules: bool = True) -> tuple[int, str, str]:\n    \"\"\"\n    Execute a shell command and return the result.\n    \n    Parameters\n    ----------\n    cmd : str\n        Command to execute\n    cwd : Path, optional\n        Working directory\n    verbose : bool\n        Print output in real-time\n    load_modules : bool\n        Prefix command with module load (default True)\n        \n    Returns\n    -------\n    tuple\n        (return_code, stdout, stderr)\n    \"\"\"\n    # Prefix with module load if configured\n    if load_modules and MODULES:\n        cmd = f\"module load {MODULES} && {cmd}\"\n    \n    if verbose:\n        print(f\"$ {cmd}\")\n        print(\"-\" * 60)\n    \n    process = subprocess.Popen(\n        cmd,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        cwd=cwd,\n        text=True\n    )\n    \n    output_lines = []\n    for line in process.stdout:\n        output_lines.append(line)\n        if verbose:\n            print(line, end=\"\")\n    \n    process.wait()\n    stdout = \"\".join(output_lines)\n    \n    if verbose:\n        print(\"-\" * 60)\n        print(f\"Return code: {process.returncode}\")\n    \n    return process.returncode, stdout, \"\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def setup_environment():\n    \"\"\"\n    Setup the benchmark environment:\n    - Create directories\n    - Sync uv dependencies\n    - Install dlio-benchmark (without pydftracer)\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"ENVIRONMENT SETUP\")\n    print(\"=\" * 60)\n    \n    # Create directories\n    print(\"\\n[1/3] Creating directories...\")\n    SCRATCH_DIR.mkdir(parents=True, exist_ok=True)\n    RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n    print(f\"  Created: {SCRATCH_DIR}\")\n    print(f\"  Created: {RESULTS_DIR}\")\n    \n    # Sync analysis dependencies\n    print(\"\\n[2/3] Syncing dependencies...\")\n    ret, _, _ = run_command(\"uv sync\", cwd=BASE_DIR)\n    if ret != 0:\n        return False\n    \n    # Install dlio-benchmark without pydftracer (which fails to build on ARM)\n    # Use --python to target the project's venv\n    print(\"\\n[3/3] Installing dlio-benchmark...\")\n    venv_python = BASE_DIR / \".venv\" / \"bin\" / \"python\"\n    ret, _, _ = run_command(\n        f\"uv pip install --python {venv_python} dlio-benchmark --no-deps && \"\n        f\"uv pip install --python {venv_python} hydra-core omegaconf pyyaml numpy mpi4py h5py\",\n        cwd=BASE_DIR\n    )\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"Environment setup complete!\")\n    print(\"=\" * 60)\n    return ret == 0\n\n\ndef cleanup_scratch():\n    \"\"\"Remove the scratch directory.\"\"\"\n    if SCRATCH_DIR.exists():\n        shutil.rmtree(SCRATCH_DIR)\n        print(f\"Cleaned up: {SCRATCH_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Initialize Environment\n",
    "\n",
    "Run this cell to setup the environment. This will:\n",
    "- Load required modules\n",
    "- Create necessary directories\n",
    "- Sync uv dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 3.4 System Information\n\nDynamically collect system specifications from the current node.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import socket\nimport platform\nimport re\n\ndef get_system_info() -> dict:\n    \"\"\"\n    Collect system information dynamically.\n    \n    Returns\n    -------\n    dict\n        Dictionary with system specifications\n    \"\"\"\n    info = {\n        \"hostname\": socket.gethostname(),\n        \"platform\": platform.platform(),\n        \"processor\": platform.processor(),\n    }\n    \n    # CPU info\n    try:\n        with open(\"/proc/cpuinfo\", \"r\") as f:\n            cpuinfo = f.read()\n        \n        # Count physical cores\n        physical_ids = set(re.findall(r\"physical id\\s*:\\s*(\\d+)\", cpuinfo))\n        cores_per_socket = len(set(re.findall(r\"core id\\s*:\\s*(\\d+)\", cpuinfo)))\n        total_cores = len(re.findall(r\"^processor\\s*:\", cpuinfo, re.MULTILINE))\n        \n        # Get CPU model name\n        model_match = re.search(r\"model name\\s*:\\s*(.+)\", cpuinfo)\n        if model_match:\n            info[\"cpu_model\"] = model_match.group(1).strip()\n        else:\n            # For ARM, try to get CPU part\n            cpu_part = re.search(r\"CPU part\\s*:\\s*(.+)\", cpuinfo)\n            info[\"cpu_model\"] = f\"ARM {cpu_part.group(1).strip()}\" if cpu_part else \"Unknown\"\n        \n        info[\"cpu_sockets\"] = len(physical_ids) if physical_ids else 1\n        info[\"cpu_cores_total\"] = total_cores\n    except Exception as e:\n        info[\"cpu_error\"] = str(e)\n    \n    # Memory info\n    try:\n        with open(\"/proc/meminfo\", \"r\") as f:\n            meminfo = f.read()\n        \n        mem_match = re.search(r\"MemTotal:\\s*(\\d+)\\s*kB\", meminfo)\n        if mem_match:\n            mem_kb = int(mem_match.group(1))\n            info[\"memory_gb\"] = round(mem_kb / 1024 / 1024, 1)\n    except Exception as e:\n        info[\"memory_error\"] = str(e)\n    \n    # GPU info (nvidia-smi)\n    try:\n        result = subprocess.run(\n            [\"nvidia-smi\", \"--query-gpu=name,memory.total,count\", \"--format=csv,noheader,nounits\"],\n            capture_output=True, text=True, timeout=10\n        )\n        if result.returncode == 0:\n            gpu_lines = result.stdout.strip().split(\"\\n\")\n            gpu_count = len(gpu_lines)\n            if gpu_lines and gpu_lines[0]:\n                parts = gpu_lines[0].split(\", \")\n                info[\"gpu_model\"] = parts[0].strip()\n                info[\"gpu_memory_mb\"] = int(parts[1].strip()) if len(parts) > 1 else 0\n                info[\"gpu_count\"] = gpu_count\n    except Exception as e:\n        info[\"gpu_info\"] = \"Not available\"\n    \n    # Storage info\n    try:\n        result = subprocess.run(\n            [\"df\", \"-h\", \"--output=size,avail,target\", \"/\"],\n            capture_output=True, text=True, timeout=10\n        )\n        if result.returncode == 0:\n            lines = result.stdout.strip().split(\"\\n\")\n            if len(lines) > 1:\n                parts = lines[1].split()\n                info[\"storage_total\"] = parts[0]\n                info[\"storage_available\"] = parts[1]\n    except Exception as e:\n        info[\"storage_error\"] = str(e)\n    \n    return info\n\n\ndef display_system_info(info: dict):\n    \"\"\"Display system information as a formatted table.\"\"\"\n    print(\"=\" * 60)\n    print(\"SYSTEM INFORMATION\")\n    print(\"=\" * 60)\n    print(f\"Hostname: {info.get('hostname', 'Unknown')}\")\n    print(f\"Platform: {info.get('platform', 'Unknown')}\")\n    print()\n    \n    # CPU\n    print(\"CPU:\")\n    print(f\"  Model: {info.get('cpu_model', 'Unknown')}\")\n    print(f\"  Sockets: {info.get('cpu_sockets', 'Unknown')}\")\n    print(f\"  Total Cores: {info.get('cpu_cores_total', 'Unknown')}\")\n    print()\n    \n    # Memory\n    print(\"Memory:\")\n    print(f\"  Total: {info.get('memory_gb', 'Unknown')} GiB\")\n    print()\n    \n    # GPU\n    if \"gpu_model\" in info:\n        print(\"GPU:\")\n        print(f\"  Model: {info.get('gpu_model', 'Unknown')}\")\n        print(f\"  Count: {info.get('gpu_count', 'Unknown')}\")\n        print(f\"  Memory per GPU: {info.get('gpu_memory_mb', 0) / 1024:.0f} GB\")\n    else:\n        print(\"GPU: Not available\")\n    print()\n    \n    # Storage\n    print(\"Storage:\")\n    print(f\"  Total: {info.get('storage_total', 'Unknown')}\")\n    print(f\"  Available: {info.get('storage_available', 'Unknown')}\")\n    print(\"=\" * 60)\n    \n    return info\n\n\n# Collect and display system info\nsystem_info = get_system_info()\ndisplay_system_info(system_info)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create a summary table for the report\ndef system_info_table(info: dict) -> pd.DataFrame:\n    \"\"\"Create a DataFrame with system specifications.\"\"\"\n    \n    gpu_spec = \"Not available\"\n    if \"gpu_model\" in info:\n        gpu_mem_gb = info.get('gpu_memory_mb', 0) / 1024\n        gpu_spec = f\"{info.get('gpu_count', 1)}x {info.get('gpu_model', 'Unknown')} ({gpu_mem_gb:.0f}GB each)\"\n    \n    data = [\n        (\"CPU\", f\"{info.get('cpu_sockets', 1)}x {info.get('cpu_model', 'Unknown')} ({info.get('cpu_cores_total', 'Unknown')} cores total)\"),\n        (\"Memory (RAM)\", f\"{info.get('memory_gb', 'Unknown')} GiB\"),\n        (\"GPU\", gpu_spec),\n        (\"Storage\", f\"{info.get('storage_total', 'Unknown')} (Available: {info.get('storage_available', 'Unknown')})\"),\n    ]\n    \n    return pd.DataFrame(data, columns=[\"Component\", \"Specification\"])\n\n\n# Display as table\nsystem_table = system_info_table(system_info)\ndisplay(system_table.style.hide(axis='index').set_properties(**{'text-align': 'left'}))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimental Design\n",
    "\n",
    "### 4.1 Load Experiment Configuration\n",
    "\n",
    "The experimental design defines all benchmark runs to execute. The `run` column indicates completion status:\n",
    "- `N` = Not yet run (pending)\n",
    "- `Y` = Completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_design() -> pd.DataFrame:\n",
    "    \"\"\"Load the experimental design CSV.\"\"\"\n",
    "    df = pd.read_csv(EXPERIMENT_FILE)\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_experiment_design(df: pd.DataFrame):\n",
    "    \"\"\"Save the experimental design CSV.\"\"\"\n",
    "    df.to_csv(EXPERIMENT_FILE, index=False)\n",
    "    print(f\"Saved experiment design to {EXPERIMENT_FILE}\")\n",
    "\n",
    "\n",
    "def get_pending_experiments(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Get experiments that haven't been run yet.\"\"\"\n",
    "    return df[df['run'] == 'N'].copy()\n",
    "\n",
    "\n",
    "def mark_experiment_complete(df: pd.DataFrame, model: str, processes: int) -> pd.DataFrame:\n",
    "    \"\"\"Mark an experiment as completed.\"\"\"\n",
    "    mask = (df['model'] == model) & (df['processes'] == processes)\n",
    "    df.loc[mask, 'run'] = 'Y'\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load experiment design\n",
    "experiment_df = load_experiment_design()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENTAL DESIGN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total experiments: {len(experiment_df)}\")\n",
    "print(f\"Completed: {(experiment_df['run'] == 'Y').sum()}\")\n",
    "print(f\"Pending: {(experiment_df['run'] == 'N').sum()}\")\n",
    "print(\"\\nExperiment matrix:\")\n",
    "experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_experiment_status(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Visualize experiment completion status.\n",
    "    \"\"\"\n",
    "    pivot = df.pivot(index='model', columns='processes', values='run')\n",
    "    pivot_numeric = pivot.replace({'Y': 1, 'N': 0})\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        pivot_numeric,\n",
    "        annot=pivot.values,\n",
    "        fmt='',\n",
    "        cmap=['#ffcccc', '#ccffcc'],\n",
    "        cbar=False,\n",
    "        linewidths=2,\n",
    "        linecolor='white',\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Experiment Status Matrix\\n(Y = Completed, N = Pending)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Number of Processes', fontsize=12)\n",
    "    ax.set_ylabel('Model', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_experiment_status(experiment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmark Execution\n",
    "\n",
    "### 5.1 Benchmark Runner\n",
    "\n",
    "Functions to execute DLIO benchmarks based on the experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(model: str, num_procs: int = 8) -> bool:\n",
    "    \"\"\"\n",
    "    Generate synthetic data for a workload.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : str\n",
    "        Workload name (e.g., 'default_custom')\n",
    "    num_procs : int\n",
    "        Number of MPI processes for data generation\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if successful\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GENERATING DATA: {model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "    mpirun -np {num_procs} \\\n",
    "        uv run --project {BASE_DIR} dlio_benchmark \\\n",
    "        --config-dir {CONFIG_DIR} \\\n",
    "        workload={model} \\\n",
    "        ++workload.workflow.generate_data=True \\\n",
    "        ++workload.workflow.train=False \\\n",
    "        ++workload.workflow.evaluation=False\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    ret, _, _ = run_command(cmd, cwd=SCRATCH_DIR)\n",
    "    return ret == 0\n",
    "\n",
    "\n",
    "def run_benchmark(model: str, num_procs: int) -> bool:\n",
    "    \"\"\"\n",
    "    Run a benchmark for a specific model and process count.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : str\n",
    "        Workload name\n",
    "    num_procs : int\n",
    "        Number of MPI processes\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if successful\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUNNING BENCHMARK: {model} with {num_procs} processes\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "    mpirun -np {num_procs} \\\n",
    "        uv run --project {BASE_DIR} dlio_benchmark \\\n",
    "        --config-dir {CONFIG_DIR} \\\n",
    "        workload={model} \\\n",
    "        ++workload.workflow.generate_data=False \\\n",
    "        ++workload.workflow.train=True \\\n",
    "        ++workload.workflow.evaluation=True\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    ret, _, _ = run_command(cmd, cwd=SCRATCH_DIR)\n",
    "    \n",
    "    if ret == 0:\n",
    "        # Copy results\n",
    "        result_src = SCRATCH_DIR / \"hydra_log\" / model\n",
    "        result_dst = RESULTS_DIR / model / str(num_procs)\n",
    "        \n",
    "        if result_src.exists():\n",
    "            result_dst.mkdir(parents=True, exist_ok=True)\n",
    "            for f in result_src.iterdir():\n",
    "                shutil.copy2(f, result_dst)\n",
    "            print(f\"\\nResults copied to: {result_dst}\")\n",
    "            \n",
    "            # Clean up hydra_log\n",
    "            shutil.rmtree(result_src)\n",
    "    \n",
    "    return ret == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Run Single Experiment\n",
    "\n",
    "Use this cell to run a single experiment. Modify the parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SINGLE EXPERIMENT - Modify these parameters\n",
    "# ============================================================\n",
    "RUN_MODEL = \"default_custom\"  # Model to run\n",
    "RUN_PROCS = 1                  # Number of processes\n",
    "GENERATE_DATA_FIRST = True     # Generate data before benchmark?\n",
    "# ============================================================\n",
    "\n",
    "# Check if already completed\n",
    "exp_row = experiment_df[(experiment_df['model'] == RUN_MODEL) & \n",
    "                        (experiment_df['processes'] == RUN_PROCS)]\n",
    "\n",
    "if not exp_row.empty and exp_row.iloc[0]['run'] == 'Y':\n",
    "    print(f\"Experiment {RUN_MODEL} with {RUN_PROCS} procs already completed.\")\n",
    "    print(\"Set run='N' in the CSV to re-run, or modify parameters above.\")\n",
    "else:\n",
    "    print(f\"Will run: {RUN_MODEL} with {RUN_PROCS} processes\")\n",
    "    print(f\"Generate data first: {GENERATE_DATA_FIRST}\")\n",
    "    print(\"\\nExecute the next cell to start the benchmark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the single experiment\n",
    "if GENERATE_DATA_FIRST:\n",
    "    if not generate_data(RUN_MODEL):\n",
    "        raise RuntimeError(f\"Data generation failed for {RUN_MODEL}\")\n",
    "\n",
    "if run_benchmark(RUN_MODEL, RUN_PROCS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUCCESS!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Update experiment status\n",
    "    experiment_df = mark_experiment_complete(experiment_df, RUN_MODEL, RUN_PROCS)\n",
    "    save_experiment_design(experiment_df)\n",
    "    \n",
    "    print(f\"\\nUpdated experiment status for {RUN_MODEL} ({RUN_PROCS} procs)\")\n",
    "else:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FAILED!\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Run All Pending Experiments\n",
    "\n",
    "This will run all experiments marked as `N` (pending) in the experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_pending_experiments(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run all pending experiments from the experimental design.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Experimental design DataFrame\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Updated DataFrame with completion status\n",
    "    \"\"\"\n",
    "    pending = get_pending_experiments(df)\n",
    "    \n",
    "    if pending.empty:\n",
    "        print(\"No pending experiments to run!\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Found {len(pending)} pending experiments\")\n",
    "    print(\"\\nPending experiments:\")\n",
    "    display(pending)\n",
    "    \n",
    "    # Group by model to generate data once per model\n",
    "    models = pending['model'].unique()\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"# Processing model: {model}\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        \n",
    "        # Generate data for this model\n",
    "        model_pending = pending[pending['model'] == model]\n",
    "        max_procs = model_pending['processes'].max()\n",
    "        \n",
    "        if not generate_data(model, num_procs=max_procs):\n",
    "            print(f\"ERROR: Data generation failed for {model}\")\n",
    "            continue\n",
    "        \n",
    "        # Run benchmarks for each process count\n",
    "        for _, row in model_pending.iterrows():\n",
    "            procs = row['processes']\n",
    "            \n",
    "            if run_benchmark(model, procs):\n",
    "                df = mark_experiment_complete(df, model, procs)\n",
    "                save_experiment_design(df)\n",
    "                print(f\"Marked {model} ({procs} procs) as complete\")\n",
    "            else:\n",
    "                print(f\"ERROR: Benchmark failed for {model} ({procs} procs)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Show pending experiments\n",
    "pending = get_pending_experiments(experiment_df)\n",
    "print(f\"Pending experiments: {len(pending)}\")\n",
    "if not pending.empty:\n",
    "    display(pending)\n",
    "    print(\"\\nExecute the next cell to run all pending experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all pending experiments\n",
    "# WARNING: This may take a long time!\n",
    "\n",
    "experiment_df = run_all_pending_experiments(experiment_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "plot_experiment_status(experiment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis\n",
    "\n",
    "### 6.1 Load Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchmark_results(results_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all benchmark summary.json files.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for file_path in glob.glob(str(results_dir / \"**/summary.json\"), recursive=True):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            summary = json.load(f)\n",
    "        \n",
    "        path_parts = Path(file_path).parts\n",
    "        try:\n",
    "            results_idx = path_parts.index('results')\n",
    "            model_name = path_parts[results_idx + 1]\n",
    "        except (ValueError, IndexError):\n",
    "            model_name = Path(file_path).parent.parent.name\n",
    "        \n",
    "        num_accelerators = summary.get(\"num_accelerators\", 0)\n",
    "        metrics = summary.get(\"metric\", {})\n",
    "        \n",
    "        data.append([\n",
    "            model_name,\n",
    "            num_accelerators,\n",
    "            metrics.get(\"train_au_mean_percentage\", 0),\n",
    "            metrics.get(\"train_au_stdev_percentage\", 0),\n",
    "            metrics.get(\"train_io_mean_MB_per_second\", 0),\n",
    "            metrics.get(\"train_io_stdev_MB_per_second\", 0)\n",
    "        ])\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        data,\n",
    "        columns=[\"model\", \"processes\", \"accelerator_usage\", \n",
    "                 \"accelerator_usage_std\", \"io_throughput\", \"io_throughput_std\"]\n",
    "    )\n",
    "    \n",
    "    return df.sort_values(by=[\"model\", \"processes\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Load results\n",
    "results_df = load_benchmark_results(RESULTS_DIR)\n",
    "print(f\"Loaded {len(results_df)} benchmark results\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Accelerator Usage vs. Number of Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accelerator_usage(df: pd.DataFrame):\n",
    "    if df.empty:\n",
    "        print(\"No data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    colors = sns.color_palette(\"husl\", n_colors=len(df[\"model\"].unique()))\n",
    "    \n",
    "    for i, model in enumerate(df[\"model\"].unique()):\n",
    "        model_df = df[df[\"model\"] == model].sort_values(\"processes\")\n",
    "        ax.errorbar(\n",
    "            model_df[\"processes\"],\n",
    "            model_df[\"accelerator_usage\"],\n",
    "            yerr=model_df[\"accelerator_usage_std\"],\n",
    "            marker='o', markersize=8, linewidth=2,\n",
    "            label=model, capsize=5, capthick=2, color=colors[i]\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel(\"Number of Processes\", fontsize=14)\n",
    "    ax.set_ylabel(\"Accelerator Usage (%)\", fontsize=14)\n",
    "    ax.set_title(\"Accelerator Usage vs. Number of Processes\", fontsize=16, fontweight='bold')\n",
    "    ax.legend(title=\"Model\", loc=\"best\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xticks(sorted(df[\"processes\"].unique()))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"accelerator_usage.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_accelerator_usage(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 I/O Throughput vs. Number of Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_io_throughput(df: pd.DataFrame):\n",
    "    if df.empty:\n",
    "        print(\"No data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    colors = sns.color_palette(\"husl\", n_colors=len(df[\"model\"].unique()))\n",
    "    \n",
    "    for i, model in enumerate(df[\"model\"].unique()):\n",
    "        model_df = df[df[\"model\"] == model].sort_values(\"processes\")\n",
    "        ax.errorbar(\n",
    "            model_df[\"processes\"],\n",
    "            model_df[\"io_throughput\"],\n",
    "            yerr=model_df[\"io_throughput_std\"],\n",
    "            marker='s', markersize=8, linewidth=2,\n",
    "            label=model, capsize=5, capthick=2, color=colors[i]\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel(\"Number of Processes\", fontsize=14)\n",
    "    ax.set_ylabel(\"I/O Throughput (MB/s)\", fontsize=14)\n",
    "    ax.set_title(\"I/O Throughput vs. Number of Processes\", fontsize=16, fontweight='bold')\n",
    "    ax.legend(title=\"Model\", loc=\"best\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xticks(sorted(df[\"processes\"].unique()))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"io_throughput.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_io_throughput(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    summary = results_df.copy()\n",
    "    summary['accelerator_usage'] = summary['accelerator_usage'].round(2)\n",
    "    summary['io_throughput'] = summary['io_throughput'].round(4)\n",
    "    summary.columns = ['Model', 'Procs', 'AU (%)', 'AU Std', 'I/O (MB/s)', 'I/O Std']\n",
    "    display(summary.style.background_gradient(subset=['AU (%)'], cmap='RdYlGn', vmin=0, vmax=100))\n",
    "else:\n",
    "    print(\"No results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup\n",
    "\n",
    "Run this cell to clean up the scratch directory when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_scratch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on the analysis of the benchmark results:\n",
    "\n",
    "1. **Accelerator Usage Patterns:** [Add observations]\n",
    "\n",
    "2. **I/O Throughput Scaling:** [Add observations]\n",
    "\n",
    "3. **Model-Specific Characteristics:** [Add observations]\n",
    "\n",
    "---\n",
    "\n",
    "**Repository:** https://github.com/HpcResearchLaboratory/perf_2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}