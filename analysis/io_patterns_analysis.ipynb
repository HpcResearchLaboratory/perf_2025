{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O Patterns and Bottlenecks in Deep Learning Workloads\n",
    "\n",
    "**Author:** Pablo Alessandro Santos Hugen  \n",
    "**Institution:** Institute of Informatics -- UFRGS  \n",
    "**Course:** Computer Systems Performance Analysis 2025/2\n",
    "\n",
    "---\n",
    "\n",
    "- Environment setup and configuration\n",
    "- Loading the experimental design\n",
    "- Running DLIO benchmarks\n",
    "- Collecting and analyzing results\n",
    "\n",
    "**Prerequisites:**\n",
    "- Allocate an interactive node: `salloc --partition=<partition> --nodes=1 --ntasks=8 --time=4:00:00`\n",
    "- Launch Jupyter from the allocated node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Context\n",
    "\n",
    "Recent years have seen growing interest in optimizations for Machine Learning and Deep Learning training and inference methods. These techniques are now used across various fields, including Large Language Models (LLMs), image recognition and classification, and many other applications.\n",
    "\n",
    "Large models often require substantial HPC infrastructures to process the enormous amounts of training data involved. In this context, **the performance of the storage and I/O subsystem is critical**.\n",
    "\n",
    "#### Traditional HPC vs. ML Workloads\n",
    "\n",
    "| Aspect | Traditional HPC | ML Workloads |\n",
    "|--------|-----------------|---------------|\n",
    "| Access Pattern | Large, sequential reads/writes | Small, random reads across numerous files |\n",
    "| Typical Use Case | Simulations with periodic checkpoints | Iterative training over dataset epochs |\n",
    "| I/O Characteristics | Predictable, burst-oriented | Continuous, irregular access patterns |\n",
    "\n",
    "### 1.2 The I/O Bottleneck Problem\n",
    "\n",
    "At large-scale distributed DL workloads:\n",
    "- **I/O can take roughly 85% of the training time** (Mohan et al., 2021)\n",
    "- Training is often one of the most expensive parts of the ML pipeline (Chowdhury et al., 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Objectives\n",
    "\n",
    "### 2.1 General Objective\n",
    "\n",
    "Understand **patterns in I/O operations and possible bottlenecks** in common Machine Learning workloads.\n",
    "\n",
    "### 2.2 Specific Objectives\n",
    "\n",
    "1. **Disk Throughput:** Understand how disk throughput varies during training between epochs, checkpoints, and when the number of training processes varies.\n",
    "\n",
    "2. **GPU Usage:** Analyze how GPU usage (%) behaves in those scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Setup\n",
    "\n",
    "### 3.1 Configuration\n",
    "\n",
    "Configure the environment variables for your cluster below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODULES = \"arch_gpu_sc/current openmpi/4.1.6.15.1\"\n",
    "BASE_DIR = Path(\"..\").resolve()\n",
    "CONFIG_DIR = BASE_DIR / \"config\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "EXPERIMENT_FILE = Path(\"experimental_design.csv\")\n",
    "SCRATCH_DIR = BASE_DIR / f\"dlio_data_{os.getpid()}\"\n",
    "\n",
    "print(f\"Modules: {MODULES}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Config directory: {CONFIG_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Scratch directory: {SCRATCH_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Helper Functions\n",
    "\n",
    "Functions for running shell commands and managing the benchmark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(cmd: str, cwd: Path = None, verbose: bool = True, load_modules: bool = True) -> tuple[int, str, str]:\n",
    "    if load_modules and MODULES:\n",
    "        cmd = f\"module load {MODULES} && {cmd}\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"$ {cmd}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        cwd=cwd,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    output_lines = []\n",
    "    for line in process.stdout:\n",
    "        output_lines.append(line)\n",
    "        if verbose:\n",
    "            print(line, end=\"\")\n",
    "    \n",
    "    process.wait()\n",
    "    stdout = \"\".join(output_lines)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Return code: {process.returncode}\")\n",
    "    \n",
    "    return process.returncode, stdout, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_environment():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ENVIRONMENT SETUP\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    venv_dir = BASE_DIR / \".venv\"\n",
    "    venv_python = venv_dir / \"bin\" / \"python\"\n",
    "    \n",
    "    if not venv_dir.exists():\n",
    "        print(\"\\n[1/4] Creating virtual environment...\")\n",
    "        ret, _, _ = run_command(f\"uv venv --python $(which python3)\", cwd=BASE_DIR, load_modules=False)\n",
    "        if ret != 0:\n",
    "            print(\"  ERROR: Failed to create venv\")\n",
    "            return False\n",
    "        \n",
    "        print(\"\\n[2/4] Installing dlio-benchmark from submodule...\")\n",
    "        ret, _, _ = run_command(f\"uv pip install --python {venv_python} ./dlio_benchmark/\", cwd=BASE_DIR, load_modules=False)\n",
    "        if ret != 0:\n",
    "            print(\"  ERROR: Failed to install dlio-benchmark\")\n",
    "            return False\n",
    "        \n",
    "        print(\"\\n[3/4] Installing analysis dependencies...\")\n",
    "        ret, _, _ = run_command(f\"uv pip install --python {venv_python} jupyter pandas matplotlib seaborn\", cwd=BASE_DIR, load_modules=False)\n",
    "        if ret != 0:\n",
    "            print(\"  WARNING: Some dependencies may have failed\")\n",
    "    else:\n",
    "        print(\"\\n[1/4] Virtual environment already exists, skipping creation...\")\n",
    "        print(\"[2/4] Skipping dlio-benchmark install...\")\n",
    "        print(\"[3/4] Skipping dependencies install...\")\n",
    "    \n",
    "    SCRATCH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Environment setup complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    return True\n",
    "\n",
    "\n",
    "def cleanup_scratch():\n",
    "    if SCRATCH_DIR.exists():\n",
    "        shutil.rmtree(SCRATCH_DIR)\n",
    "        print(f\"Cleaned up: {SCRATCH_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Initialize Environment\n",
    "\n",
    "Run this cell to setup the environment. This will:\n",
    "- Load required modules\n",
    "- Create necessary directories\n",
    "- Sync uv dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 System Information\n",
    "\n",
    "Dynamically collect system specifications from the current node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import platform\n",
    "import re\n",
    "\n",
    "def get_system_info() -> dict:\n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"processor\": platform.processor(),\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(\"/proc/cpuinfo\", \"r\") as f:\n",
    "            cpuinfo = f.read()\n",
    "        \n",
    "        physical_ids = set(re.findall(r\"physical id\\s*:\\s*(\\d+)\", cpuinfo))\n",
    "        cores_per_socket = len(set(re.findall(r\"core id\\s*:\\s*(\\d+)\", cpuinfo)))\n",
    "        total_cores = len(re.findall(r\"^processor\\s*:\", cpuinfo, re.MULTILINE))\n",
    "        model_match = re.search(r\"model name\\s*:\\s*(.+)\", cpuinfo)\n",
    "        if model_match:\n",
    "            info[\"cpu_model\"] = model_match.group(1).strip()\n",
    "        else:\n",
    "            cpu_part = re.search(r\"CPU part\\s*:\\s*(.+)\", cpuinfo)\n",
    "            info[\"cpu_model\"] = f\"ARM {cpu_part.group(1).strip()}\" if cpu_part else \"Unknown\"\n",
    "        \n",
    "        info[\"cpu_sockets\"] = len(physical_ids) if physical_ids else 1\n",
    "        info[\"cpu_cores_total\"] = total_cores\n",
    "    except Exception as e:\n",
    "        info[\"cpu_error\"] = str(e)\n",
    "    \n",
    "    try:\n",
    "        with open(\"/proc/meminfo\", \"r\") as f:\n",
    "            meminfo = f.read()\n",
    "        \n",
    "        mem_match = re.search(r\"MemTotal:\\s*(\\d+)\\s*kB\", meminfo)\n",
    "        if mem_match:\n",
    "            mem_kb = int(mem_match.group(1))\n",
    "            info[\"memory_gb\"] = round(mem_kb / 1024 / 1024, 1)\n",
    "    except Exception as e:\n",
    "        info[\"memory_error\"] = str(e)\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name,memory.total,count\", \"--format=csv,noheader,nounits\"],\n",
    "            capture_output=True, text=True, timeout=10\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            gpu_lines = result.stdout.strip().split(\"\\n\")\n",
    "            gpu_count = len(gpu_lines)\n",
    "            if gpu_lines and gpu_lines[0]:\n",
    "                parts = gpu_lines[0].split(\", \")\n",
    "                info[\"gpu_model\"] = parts[0].strip()\n",
    "                info[\"gpu_memory_mb\"] = int(parts[1].strip()) if len(parts) > 1 else 0\n",
    "                info[\"gpu_count\"] = gpu_count\n",
    "    except Exception as e:\n",
    "        info[\"gpu_info\"] = \"Not available\"\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"df\", \"-h\", \"--output=size,avail,target\", \"/\"],\n",
    "            capture_output=True, text=True, timeout=10\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            lines = result.stdout.strip().split(\"\\n\")\n",
    "            if len(lines) > 1:\n",
    "                parts = lines[1].split()\n",
    "                info[\"storage_total\"] = parts[0]\n",
    "                info[\"storage_available\"] = parts[1]\n",
    "    except Exception as e:\n",
    "        info[\"storage_error\"] = str(e)\n",
    "    \n",
    "    return info\n",
    "\n",
    "\n",
    "def display_system_info(info: dict):\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SYSTEM INFORMATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Hostname: {info.get('hostname', 'Unknown')}\")\n",
    "    print(f\"Platform: {info.get('platform', 'Unknown')}\")\n",
    "    print()\n",
    "    print(\"CPU:\")\n",
    "    print(f\"  Model: {info.get('cpu_model', 'Unknown')}\")\n",
    "    print(f\"  Sockets: {info.get('cpu_sockets', 'Unknown')}\")\n",
    "    print(f\"  Total Cores: {info.get('cpu_cores_total', 'Unknown')}\")\n",
    "    print()\n",
    "    print(\"Memory:\")\n",
    "    print(f\"  Total: {info.get('memory_gb', 'Unknown')} GiB\")\n",
    "    print()\n",
    "    if \"gpu_model\" in info:\n",
    "        print(\"GPU:\")\n",
    "        print(f\"  Model: {info.get('gpu_model', 'Unknown')}\")\n",
    "        print(f\"  Count: {info.get('gpu_count', 'Unknown')}\")\n",
    "        print(f\"  Memory per GPU: {info.get('gpu_memory_mb', 0) / 1024:.0f} GB\")\n",
    "    else:\n",
    "        print(\"GPU: Not available\")\n",
    "    print()\n",
    "    print(\"Storage:\")\n",
    "    print(f\"  Total: {info.get('storage_total', 'Unknown')}\")\n",
    "    print(f\"  Available: {info.get('storage_available', 'Unknown')}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return info\n",
    "\n",
    "system_info = get_system_info()\n",
    "display_system_info(system_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_info_table(info: dict) -> pd.DataFrame:\n",
    "    gpu_spec = \"Not available\"\n",
    "    if \"gpu_model\" in info:\n",
    "        gpu_mem_gb = info.get('gpu_memory_mb', 0) / 1024\n",
    "        gpu_spec = f\"{info.get('gpu_count', 1)}x {info.get('gpu_model', 'Unknown')} ({gpu_mem_gb:.0f}GB each)\"\n",
    "    \n",
    "    data = [\n",
    "        (\"CPU\", f\"{info.get('cpu_sockets', 1)}x {info.get('cpu_model', 'Unknown')} ({info.get('cpu_cores_total', 'Unknown')} cores total)\"),\n",
    "        (\"Memory (RAM)\", f\"{info.get('memory_gb', 'Unknown')} GiB\"),\n",
    "        (\"GPU\", gpu_spec),\n",
    "        (\"Storage\", f\"{info.get('storage_total', 'Unknown')} (Available: {info.get('storage_available', 'Unknown')})\"),\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(data, columns=[\"Component\", \"Specification\"])\n",
    "\n",
    "system_table = system_info_table(system_info)\n",
    "display(system_table.style.hide(axis='index').set_properties(**{'text-align': 'left'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimental Design\n",
    "\n",
    "### 4.1 Load Experiment Configuration\n",
    "\n",
    "The experimental design defines all benchmark runs to execute. The `run` column indicates completion status:\n",
    "- `N` = Not yet run (pending)\n",
    "- `Y` = Completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_design() -> pd.DataFrame:\n",
    "    df = pd.read_csv(EXPERIMENT_FILE, comment='#')\n",
    "    if 'overrides' in df.columns:\n",
    "        df['overrides'] = df['overrides'].fillna('')\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_experiment_design(df: pd.DataFrame):\n",
    "    df.to_csv(EXPERIMENT_FILE, index=False)\n",
    "    print(f\"Saved experiment design to {EXPERIMENT_FILE}\")\n",
    "\n",
    "\n",
    "def get_pending_experiments(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df[df['run'] == 'N'].copy()\n",
    "\n",
    "\n",
    "def get_experiment_id(row: pd.Series) -> str:\n",
    "    overrides = row.get('overrides', '')\n",
    "    if overrides:\n",
    "        param = overrides.split('=')[0].split('.')[-1]\n",
    "        value = overrides.split('=')[1] if '=' in overrides else ''\n",
    "        return f\"{row['processes']}_{param}_{value}\"\n",
    "    return str(row['processes'])\n",
    "\n",
    "\n",
    "def mark_experiment_complete(df: pd.DataFrame, model: str, processes: int, overrides: str = '') -> pd.DataFrame:\n",
    "    mask = (df['model'] == model) & (df['processes'] == processes)\n",
    "    if 'overrides' in df.columns:\n",
    "        mask = mask & (df['overrides'] == overrides)\n",
    "    df.loc[mask, 'run'] = 'Y'\n",
    "    return df\n",
    "\n",
    "experiment_df = load_experiment_design()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENTAL DESIGN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total experiments: {len(experiment_df)}\")\n",
    "print(f\"Completed: {(experiment_df['run'] == 'Y').sum()}\")\n",
    "print(f\"Pending: {(experiment_df['run'] == 'N').sum()}\")\n",
    "print(\"\\nExperiment matrix:\")\n",
    "experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_experiment_status(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    df['category'] = df['overrides'].apply(\n",
    "        lambda x: x.split('.')[-1].split('=')[0] if x else 'baseline'\n",
    "    )\n",
    "    \n",
    "    summary = df.groupby(['category', 'run']).size().unstack(fill_value=0)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    status_counts = df['run'].value_counts()\n",
    "    colors = ['#ccffcc' if s == 'Y' else '#ffcccc' for s in status_counts.index]\n",
    "    axes[0].pie(status_counts, labels=['Completed' if s == 'Y' else 'Pending' for s in status_counts.index],\n",
    "                colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0].set_title('Overall Experiment Status', fontsize=14, fontweight='bold')\n",
    "    if 'Y' not in summary.columns:\n",
    "        summary['Y'] = 0\n",
    "    if 'N' not in summary.columns:\n",
    "        summary['N'] = 0\n",
    "    \n",
    "    x = range(len(summary))\n",
    "    width = 0.35\n",
    "    axes[1].bar([i - width/2 for i in x], summary['N'], width, label='Pending', color='#ffcccc')\n",
    "    axes[1].bar([i + width/2 for i in x], summary['Y'], width, label='Completed', color='#ccffcc')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(summary.index, rotation=45, ha='right')\n",
    "    axes[1].set_ylabel('Number of Experiments')\n",
    "    axes[1].set_title('Experiments by Parameter Category', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_experiment_status(experiment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmark Execution\n",
    "\n",
    "### 5.1 Benchmark Runner\n",
    "\n",
    "Functions to execute DLIO benchmarks based on the experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(model: str, num_procs: int = 8, overrides: str = '') -> bool:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GENERATING DATA: {model}\")\n",
    "    if overrides:\n",
    "        print(f\"Overrides: {overrides}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "    mpirun -np {num_procs} \\\n",
    "        uv run --project {BASE_DIR} dlio_benchmark \\\n",
    "        --config-dir {CONFIG_DIR} \\\n",
    "        workload={model} \\\n",
    "        ++workload.workflow.generate_data=True \\\n",
    "        ++workload.workflow.train=False \\\n",
    "        ++workload.workflow.evaluation=False \\\n",
    "        {overrides}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    ret, _, _ = run_command(cmd, cwd=SCRATCH_DIR)\n",
    "    return ret == 0\n",
    "\n",
    "\n",
    "def run_benchmark(model: str, num_procs: int, overrides: str = '', experiment_id: str = None) -> bool:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUNNING BENCHMARK: {model} with {num_procs} processes\")\n",
    "    if overrides:\n",
    "        print(f\"Overrides: {overrides}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    cmd = f\"\"\"\n",
    "    mpirun -np {num_procs} \\\n",
    "        uv run --project {BASE_DIR} dlio_benchmark \\\n",
    "        --config-dir {CONFIG_DIR} \\\n",
    "        workload={model} \\\n",
    "        ++workload.workflow.generate_data=False \\\n",
    "        ++workload.workflow.train=True \\\n",
    "        ++workload.workflow.evaluation=True \\\n",
    "        {overrides}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    ret, _, _ = run_command(cmd, cwd=SCRATCH_DIR)\n",
    "    \n",
    "    if ret == 0:\n",
    "        result_src = SCRATCH_DIR / \"hydra_log\" / model\n",
    "        folder_name = experiment_id if experiment_id else str(num_procs)\n",
    "        result_dst = RESULTS_DIR / model / folder_name\n",
    "        \n",
    "        if result_src.exists():\n",
    "            if result_dst.exists():\n",
    "                shutil.rmtree(result_dst)\n",
    "            shutil.copytree(result_src, result_dst)\n",
    "            print(f\"\\nResults copied to: {result_dst}\")\n",
    "            shutil.rmtree(result_src)\n",
    "    \n",
    "    return ret == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Run Single Experiment\n",
    "\n",
    "Use this cell to run a single experiment. Modify the parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RUN_MODEL = \"unet3d_h100_custom\"\n",
    "RUN_PROCS = 4\n",
    "RUN_OVERRIDES = \"\"\n",
    "GENERATE_DATA_FIRST = True\n",
    "\n",
    "exp_row = experiment_df[\n",
    "    (experiment_df['model'] == RUN_MODEL) & \n",
    "    (experiment_df['processes'] == RUN_PROCS) &\n",
    "    (experiment_df['overrides'] == RUN_OVERRIDES)\n",
    "]\n",
    "\n",
    "if not exp_row.empty and exp_row.iloc[0]['run'] == 'Y':\n",
    "    print(f\"Experiment {RUN_MODEL} with {RUN_PROCS} procs already completed.\")\n",
    "    print(\"Set run='N' in the CSV to re-run, or modify parameters above.\")\n",
    "else:\n",
    "    print(f\"Will run: {RUN_MODEL} with {RUN_PROCS} processes\")\n",
    "    if RUN_OVERRIDES:\n",
    "        print(f\"Overrides: {RUN_OVERRIDES}\")\n",
    "    print(f\"Generate data first: {GENERATE_DATA_FIRST}\")\n",
    "    print(\"\\nExecute the next cell to start the benchmark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = get_experiment_id(pd.Series({\n",
    "    'processes': RUN_PROCS,\n",
    "    'overrides': RUN_OVERRIDES\n",
    "}))\n",
    "\n",
    "if GENERATE_DATA_FIRST:\n",
    "    if not generate_data(RUN_MODEL, RUN_PROCS, RUN_OVERRIDES):\n",
    "        raise RuntimeError(f\"Data generation failed for {RUN_MODEL}\")\n",
    "\n",
    "if run_benchmark(RUN_MODEL, RUN_PROCS, RUN_OVERRIDES, experiment_id):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUCCESS!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    experiment_df = mark_experiment_complete(experiment_df, RUN_MODEL, RUN_PROCS, RUN_OVERRIDES)\n",
    "    save_experiment_design(experiment_df)\n",
    "    \n",
    "    print(f\"\\nUpdated experiment status for {RUN_MODEL} ({experiment_id})\")\n",
    "else:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FAILED!\")\n",
    "    print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Run All Pending Experiments\n",
    "\n",
    "This will run all experiments marked as `N` (pending) in the experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_pending_experiments(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    pending = get_pending_experiments(df)\n",
    "    \n",
    "    if pending.empty:\n",
    "        print(\"No pending experiments to run!\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Found {len(pending)} pending experiments\")\n",
    "    print(\"\\nPending experiments:\")\n",
    "    display(pending)\n",
    "    \n",
    "    models = pending['model'].unique()\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"# Processing model: {model}\")\n",
    "        print(f\"{'#'*60}\")\n",
    "        \n",
    "        model_pending = pending[pending['model'] == model]\n",
    "        baseline_pending = model_pending[~model_pending['overrides'].str.contains('format|record_length', na=False)]\n",
    "        if not baseline_pending.empty:\n",
    "            max_procs = baseline_pending['processes'].max()\n",
    "            if not generate_data(model, num_procs=max_procs):\n",
    "                print(f\"ERROR: Data generation failed for {model}\")\n",
    "                continue\n",
    "        \n",
    "        for idx, row in model_pending.iterrows():\n",
    "            procs = row['processes']\n",
    "            overrides = row.get('overrides', '')\n",
    "            experiment_id = get_experiment_id(row)\n",
    "            \n",
    "            if 'format' in overrides or 'record_length' in overrides:\n",
    "                if not generate_data(model, num_procs=procs, overrides=overrides):\n",
    "                    print(f\"ERROR: Data generation failed for {model} with {overrides}\")\n",
    "                    continue\n",
    "            \n",
    "            if run_benchmark(model, procs, overrides, experiment_id):\n",
    "                df = mark_experiment_complete(df, model, procs, overrides)\n",
    "                save_experiment_design(df)\n",
    "                print(f\"Marked {model} ({experiment_id}) as complete\")\n",
    "            else:\n",
    "                print(f\"ERROR: Benchmark failed for {model} ({experiment_id})\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "pending = get_pending_experiments(experiment_df)\n",
    "print(f\"Pending experiments: {len(pending)}\")\n",
    "if not pending.empty:\n",
    "    display(pending)\n",
    "    print(\"\\nExecute the next cell to run all pending experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df = run_all_pending_experiments(experiment_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "plot_experiment_status(experiment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis\n",
    "\n",
    "### 6.1 Load Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchmark_results(results_dir: Path) -> pd.DataFrame:\n",
    "    data = []\n",
    "    \n",
    "    for file_path in glob.glob(str(results_dir / \"**/summary.json\"), recursive=True):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            summary = json.load(f)\n",
    "        \n",
    "        path_parts = Path(file_path).parts\n",
    "        try:\n",
    "            results_idx = path_parts.index('results')\n",
    "            model_name = path_parts[results_idx + 1]\n",
    "            experiment_id = path_parts[results_idx + 2]\n",
    "        except (ValueError, IndexError):\n",
    "            model_name = Path(file_path).parent.parent.name\n",
    "            experiment_id = Path(file_path).parent.name\n",
    "        \n",
    "        parts = experiment_id.split('_')\n",
    "        try:\n",
    "            processes = int(parts[0])\n",
    "        except ValueError:\n",
    "            processes = 0\n",
    "        \n",
    "        if len(parts) > 1:\n",
    "            param_name = parts[1]\n",
    "            param_value = '_'.join(parts[2:]) if len(parts) > 2 else ''\n",
    "        else:\n",
    "            param_name = 'baseline'\n",
    "            param_value = ''\n",
    "        \n",
    "        num_accelerators = summary.get(\"num_accelerators\", 0)\n",
    "        metrics = summary.get(\"metric\", {})\n",
    "        \n",
    "        data.append({\n",
    "            'model': model_name,\n",
    "            'experiment_id': experiment_id,\n",
    "            'processes': processes,\n",
    "            'parameter': param_name,\n",
    "            'value': param_value,\n",
    "            'accelerator_usage': metrics.get(\"train_au_mean_percentage\", 0),\n",
    "            'accelerator_usage_std': metrics.get(\"train_au_stdev_percentage\", 0),\n",
    "            'io_throughput': metrics.get(\"train_io_mean_MB_per_second\", 0),\n",
    "            'io_throughput_std': metrics.get(\"train_io_stdev_MB_per_second\", 0)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    if not df.empty:\n",
    "        before_count = len(df)\n",
    "        df = df[~((df['accelerator_usage'] == 0) & (df['io_throughput'] == 0))]\n",
    "        filtered_count = before_count - len(df)\n",
    "        if filtered_count > 0:\n",
    "            print(f\"Filtered out {filtered_count} failed runs (AU=0 and I/O=0)\")\n",
    "        \n",
    "        df = df.sort_values(by=[\"model\", \"parameter\", \"processes\"]).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "results_df = load_benchmark_results(RESULTS_DIR)\n",
    "print(f\"Loaded {len(results_df)} valid benchmark results\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Accelerator Usage vs. Number of Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accelerator_usage(df: pd.DataFrame):\n",
    "    if df.empty:\n",
    "        print(\"No data to plot\")\n",
    "        return\n",
    "    \n",
    "    parameters = df['parameter'].unique()\n",
    "    n_params = len(parameters)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, min(n_params, 3), figsize=(6 * min(n_params, 3), 6), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, param in enumerate(parameters[:3]):\n",
    "        ax = axes[idx]\n",
    "        param_df = df[df['parameter'] == param]\n",
    "        \n",
    "        if param == 'baseline':\n",
    "            param_df = param_df.sort_values('processes')\n",
    "            ax.errorbar(\n",
    "                param_df['processes'], param_df['accelerator_usage'],\n",
    "                yerr=param_df['accelerator_usage_std'],\n",
    "                marker='o', markersize=8, linewidth=2, capsize=5\n",
    "            )\n",
    "            ax.set_xlabel('Number of Processes')\n",
    "        else:\n",
    "            ax.bar(param_df['value'].astype(str), param_df['accelerator_usage'],\n",
    "                   yerr=param_df['accelerator_usage_std'], capsize=5)\n",
    "            ax.set_xlabel(param)\n",
    "            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        ax.set_ylabel('Accelerator Usage (%)')\n",
    "        ax.set_title(f'AU: {param}', fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"accelerator_usage.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "   \n",
    "    if n_params > 3:\n",
    "        remaining = parameters[3:]\n",
    "        fig2, axes2 = plt.subplots(1, len(remaining), figsize=(6 * len(remaining), 6), squeeze=False)\n",
    "        axes2 = axes2.flatten()\n",
    "        for idx, param in enumerate(remaining):\n",
    "            ax = axes2[idx]\n",
    "            param_df = df[df['parameter'] == param]\n",
    "            ax.bar(param_df['value'].astype(str), param_df['accelerator_usage'],\n",
    "                   yerr=param_df['accelerator_usage_std'], capsize=5)\n",
    "            ax.set_xlabel(param)\n",
    "            ax.set_ylabel('Accelerator Usage (%)')\n",
    "            ax.set_title(f'AU: {param}', fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_ylim(0, 100)\n",
    "            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_accelerator_usage(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 I/O Throughput vs. Number of Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_io_throughput(df: pd.DataFrame):\n",
    "    if df.empty:\n",
    "        print(\"No data to plot\")\n",
    "        return\n",
    "    \n",
    "    parameters = df['parameter'].unique()\n",
    "    n_params = len(parameters)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, min(n_params, 3), figsize=(6 * min(n_params, 3), 6), squeeze=False)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, param in enumerate(parameters[:3]):\n",
    "        ax = axes[idx]\n",
    "        param_df = df[df['parameter'] == param]\n",
    "        \n",
    "        if param == 'baseline':\n",
    "            param_df = param_df.sort_values('processes')\n",
    "            ax.errorbar(\n",
    "                param_df['processes'], param_df['io_throughput'],\n",
    "                yerr=param_df['io_throughput_std'],\n",
    "                marker='s', markersize=8, linewidth=2, capsize=5, color='#2ecc71'\n",
    "            )\n",
    "            ax.set_xlabel('Number of Processes')\n",
    "        else:\n",
    "            ax.bar(param_df['value'].astype(str), param_df['io_throughput'],\n",
    "                   yerr=param_df['io_throughput_std'], capsize=5, color='#2ecc71')\n",
    "            ax.set_xlabel(param)\n",
    "            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        ax.set_ylabel('I/O Throughput (MB/s)')\n",
    "        ax.set_title(f'I/O: {param}', fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"io_throughput.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    if n_params > 3:\n",
    "        remaining = parameters[3:]\n",
    "        fig2, axes2 = plt.subplots(1, len(remaining), figsize=(6 * len(remaining), 6), squeeze=False)\n",
    "        axes2 = axes2.flatten()\n",
    "        for idx, param in enumerate(remaining):\n",
    "            ax = axes2[idx]\n",
    "            param_df = df[df['parameter'] == param]\n",
    "            ax.bar(param_df['value'].astype(str), param_df['io_throughput'],\n",
    "                   yerr=param_df['io_throughput_std'], capsize=5, color='#2ecc71')\n",
    "            ax.set_xlabel(param)\n",
    "            ax.set_ylabel('I/O Throughput (MB/s)')\n",
    "            ax.set_title(f'I/O: {param}', fontweight='bold')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_io_throughput(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    summary = results_df.copy()\n",
    "    summary['accelerator_usage'] = summary['accelerator_usage'].round(2)\n",
    "    summary['io_throughput'] = summary['io_throughput'].round(2)\n",
    "    \n",
    "    display_cols = ['model', 'parameter', 'value', 'processes', 'accelerator_usage', 'io_throughput']\n",
    "    summary = summary[display_cols]\n",
    "    summary.columns = ['Model', 'Parameter', 'Value', 'Procs', 'AU (%)', 'I/O (MB/s)']\n",
    "    \n",
    "    display(summary.style.background_gradient(subset=['AU (%)'], cmap='RdYlGn', vmin=0, vmax=100)\n",
    "                        .background_gradient(subset=['I/O (MB/s)'], cmap='Blues'))\n",
    "    \n",
    "    output_csv = RESULTS_DIR / \"benchmark_results.csv\"\n",
    "    summary.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nResults saved to: {output_csv}\")\n",
    "else:\n",
    "    print(\"No results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup\n",
    "\n",
    "Run this cell to clean up the scratch directory when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_scratch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on the analysis of the UNet3D benchmark results on the NVIDIA GH200 SuperChip:\n",
    "\n",
    "#### 1. Process Scaling\n",
    "\n",
    "| Processes | AU (%) | I/O (MB/s) |\n",
    "|-----------|--------|------------|\n",
    "| 1 | 33.67 | 974 |\n",
    "| 2 | 36.37 | 2,001 |\n",
    "| 4 | 58.85 | 5,699 |\n",
    "| 6 | **86.52** | **10,475** |\n",
    "| 8 | 77.94 | 9,437 |\n",
    "\n",
    "- **Optimal scaling at 6 processes**, achieving peak AU (86.5%) and I/O throughput (10.5 GB/s)\n",
    "- Performance degrades at 8 processes, likely due to resource contention or memory bandwidth saturation\n",
    "- I/O throughput scales nearly linearly from 1-6 processes (~10x improvement)\n",
    "\n",
    "#### 2. Data Format Impact\n",
    "\n",
    "| Format | AU (%) | I/O (MB/s) |\n",
    "|--------|--------|------------|\n",
    "| NPZ (baseline) | 58.85 | 5,699 |\n",
    "| PNG | 21.08 | 2,042 |\n",
    "| HDF5 | **97.53** | **9,447** |\n",
    "\n",
    "- **HDF5 is the optimal format**, delivering 97.5% AU and 66% higher I/O than NPZ\n",
    "- PNG performs poorly (21% AU) due to decompression overhead\n",
    "\n",
    "#### 3. Read Threads\n",
    "\n",
    "| Threads | AU (%) | I/O (MB/s) |\n",
    "|---------|--------|------------|\n",
    "| 1 | 27.51 | 2,664 |\n",
    "| 8 | **80.87** | **7,833** |\n",
    "| 16 | 80.50 | 7,797 |\n",
    "\n",
    "- Increasing read threads from 1→8 improves AU by **3x** (27% → 81%)\n",
    "- Diminishing returns beyond 8 threads — 16 threads shows no improvement\n",
    "\n",
    "#### 4. Batch Size Trade-offs\n",
    "\n",
    "| Batch Size | AU (%) | I/O (MB/s) |\n",
    "|------------|--------|------------|\n",
    "| 1 | **98.44** | 1,661 |\n",
    "| 4 | 62.36 | 3,834 |\n",
    "| 14 | 62.26 | **7,538** |\n",
    "\n",
    "- **Small batches (1)**: Maximize AU (98%) but limit I/O throughput — compute-bound\n",
    "- **Large batches (14)**: Higher I/O throughput but similar AU to medium batches\n",
    "- Trade-off: Choose batch size based on whether workload is compute-bound or I/O-bound\n",
    "\n",
    "#### 5. Shuffling Impact\n",
    "\n",
    "| Shuffle Type | AU (%) | I/O (MB/s) |\n",
    "|--------------|--------|------------|\n",
    "| File shuffle off | 57.64 | 5,583 |\n",
    "| File shuffle random | 57.19 | 5,539 |\n",
    "| Sample shuffle off | 57.58 | 5,577 |\n",
    "| Sample shuffle random | 58.00 | 5,618 |\n",
    "\n",
    "- **Shuffling has negligible impact** on performance (~1% difference)\n",
    "- Random access patterns do not significantly degrade I/O on this storage system\n",
    "- Safe to enable shuffling for training quality without performance penalty\n",
    "\n",
    "#### 6. Record Size\n",
    "\n",
    "| Record Size | AU (%) | I/O (MB/s) |\n",
    "|-------------|--------|------------|\n",
    "| 10 MB | 72.87 | 505 |\n",
    "| 512 MB | 19.83 | 7,034 |\n",
    "\n",
    "- **Larger records favor I/O throughput** (14x higher with 512MB vs 10MB records)\n",
    "- **Smaller records favor AU** (72% vs 20%)\n",
    "- Storage system performs better with large sequential reads\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Scale to 6 processes** for optimal resource utilization on GH200\n",
    "2. **Use HDF5 format** — provides best AU and I/O performance\n",
    "3. **Configure 8 read threads** per process for parallel I/O\n",
    "4. **Batch size selection** depends on workload characteristics (compute vs I/O bound)\n",
    "5. **Shuffling is safe** to use without performance degradation\n",
    "\n",
    "---\n",
    "\n",
    "**Repository:** https://github.com/HpcResearchLaboratory/perf_2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
