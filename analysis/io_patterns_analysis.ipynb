{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O Patterns and Bottlenecks in Deep Learning Workloads\n",
    "\n",
    "**Author:** Pablo Alessandro Santos Hugen  \n",
    "**Institution:** Institute of Informatics -- UFRGS  \n",
    "**Course:** Computer Systems Performance Analysis 2025/2\n",
    "\n",
    "---\n",
    "\n",
    "This notebook serves as the **central controller** for the entire experimental workflow:\n",
    "1. Environment setup and configuration\n",
    "2. Loading the experimental design\n",
    "3. Running DLIO benchmarks\n",
    "4. Collecting and analyzing results\n",
    "\n",
    "**Prerequisites:**\n",
    "- Allocate an interactive node: `salloc --partition=<partition> --nodes=1 --ntasks=8 --time=4:00:00`\n",
    "- Launch Jupyter from the allocated node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Context\n",
    "\n",
    "Recent years have seen growing interest in optimizations for Machine Learning and Deep Learning training and inference methods. These techniques are now used across various fields, including Large Language Models (LLMs), image recognition and classification, and many other applications.\n",
    "\n",
    "Large models often require substantial HPC infrastructures to process the enormous amounts of training data involved. In this context, **the performance of the storage and I/O subsystem is critical**.\n",
    "\n",
    "#### Traditional HPC vs. ML Workloads\n",
    "\n",
    "| Aspect | Traditional HPC | ML Workloads |\n",
    "|--------|-----------------|---------------|\n",
    "| Access Pattern | Large, sequential reads/writes | Small, random reads across numerous files |\n",
    "| Typical Use Case | Simulations with periodic checkpoints | Iterative training over dataset epochs |\n",
    "| I/O Characteristics | Predictable, burst-oriented | Continuous, irregular access patterns |\n",
    "\n",
    "### 1.2 The I/O Bottleneck Problem\n",
    "\n",
    "At large-scale distributed DL workloads:\n",
    "- **I/O can take roughly 85% of the training time** (Mohan et al., 2021)\n",
    "- Training is often one of the most expensive parts of the ML pipeline (Chowdhury et al., 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Objectives\n",
    "\n",
    "### 2.1 General Objective\n",
    "\n",
    "Understand **patterns in I/O operations and possible bottlenecks** in common Machine Learning workloads.\n",
    "\n",
    "### 2.2 Specific Objectives\n",
    "\n",
    "1. **Disk Throughput:** Understand how disk throughput varies during training between epochs, checkpoints, and when the number of training processes varies.\n",
    "\n",
    "2. **GPU Usage:** Analyze how GPU usage (%) behaves in those scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Setup\n",
    "\n",
    "### 3.1 Configuration\n",
    "\n",
    "Configure the environment variables for your cluster below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#==============================================================================\n# CONFIGURATION - Modify these for your cluster\n#==============================================================================\n\n# Environment modules to load (same as bench.slurm)\nMODULES = \"arch_gpu_sc/current openmpi/4.1.6.15.1\"\n\n# Paths\nBASE_DIR = Path(\"..\").resolve()\nCONFIG_DIR = BASE_DIR / \"config\"\nRESULTS_DIR = BASE_DIR / \"results\"\nEXPERIMENT_FILE = Path(\"experimental_design.csv\")\n\n# Create scratch directory for benchmark data\nSCRATCH_DIR = BASE_DIR / f\"dlio_data_{os.getpid()}\"\n\nprint(f\"Modules: {MODULES}\")\nprint(f\"Base directory: {BASE_DIR}\")\nprint(f\"Config directory: {CONFIG_DIR}\")\nprint(f\"Results directory: {RESULTS_DIR}\")\nprint(f\"Scratch directory: {SCRATCH_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Helper Functions\n",
    "\n",
    "Functions for running shell commands and managing the benchmark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_command(cmd: str, cwd: Path = None, verbose: bool = True, load_modules: bool = True) -> tuple[int, str, str]:\n    \"\"\"\n    Execute a shell command and return the result.\n    \n    Parameters\n    ----------\n    cmd : str\n        Command to execute\n    cwd : Path, optional\n        Working directory\n    verbose : bool\n        Print output in real-time\n    load_modules : bool\n        Prefix command with module load (default True)\n        \n    Returns\n    -------\n    tuple\n        (return_code, stdout, stderr)\n    \"\"\"\n    # Prefix with module load if configured\n    if load_modules and MODULES:\n        cmd = f\"module load {MODULES} && {cmd}\"\n    \n    if verbose:\n        print(f\"$ {cmd}\")\n        print(\"-\" * 60)\n    \n    process = subprocess.Popen(\n        cmd,\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        cwd=cwd,\n        text=True\n    )\n    \n    output_lines = []\n    for line in process.stdout:\n        output_lines.append(line)\n        if verbose:\n            print(line, end=\"\")\n    \n    process.wait()\n    stdout = \"\".join(output_lines)\n    \n    if verbose:\n        print(\"-\" * 60)\n        print(f\"Return code: {process.returncode}\")\n    \n    return process.returncode, stdout, \"\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def setup_environment():\n    \"\"\"\n    Setup the benchmark environment:\n    - Create venv if not present\n    - Install dlio-benchmark from submodule\n    - Create dftracer mock\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"ENVIRONMENT SETUP\")\n    print(\"=\" * 60)\n    \n    venv_dir = BASE_DIR / \".venv\"\n    venv_python = venv_dir / \"bin\" / \"python\"\n    \n    # Create venv if not present\n    if not venv_dir.exists():\n        print(\"\\n[1/4] Creating virtual environment...\")\n        ret, _, _ = run_command(f\"uv venv --python $(which python3)\", cwd=BASE_DIR, load_modules=False)\n        if ret != 0:\n            print(\"  ERROR: Failed to create venv\")\n            return False\n        \n        print(\"\\n[2/4] Installing dlio-benchmark from submodule...\")\n        ret, _, _ = run_command(f\"uv pip install --python {venv_python} ./dlio_benchmark/\", cwd=BASE_DIR, load_modules=False)\n        if ret != 0:\n            print(\"  ERROR: Failed to install dlio-benchmark\")\n            return False\n        \n        print(\"\\n[3/4] Installing analysis dependencies...\")\n        ret, _, _ = run_command(f\"uv pip install --python {venv_python} jupyter pandas matplotlib seaborn\", cwd=BASE_DIR, load_modules=False)\n        if ret != 0:\n            print(\"  WARNING: Some dependencies may have failed\")\n    else:\n        print(\"\\n[1/4] Virtual environment already exists, skipping creation...\")\n        print(\"[2/4] Skipping dlio-benchmark install...\")\n        print(\"[3/4] Skipping dependencies install...\")\n    \n    # Create mock dftracer to avoid Profile() error\n    print(\"\\n[4/4] Creating dftracer mock...\")\n    dftracer_dir = venv_dir / \"lib64\" / \"python3.9\" / \"site-packages\" / \"dftracer\"\n    dftracer_dir.mkdir(parents=True, exist_ok=True)\n    \n    mock_code = '''class Profile:\n    def __init__(self, *args, **kwargs): pass\n    def __enter__(self): return self\n    def __exit__(self, *args): pass\n    def log(self, *args, **kwargs): pass\n    def log_event(self, *args, **kwargs): pass\n    def iter(self, iterable, *args, **kwargs): return iter(iterable) if iterable else iter([])\n\ndef dft_fn(*args, **kwargs):\n    def decorator(func):\n        return func\n    return decorator\n'''\n    (dftracer_dir / \"__init__.py\").write_text(mock_code)\n    print(f\"  Created: {dftracer_dir / '__init__.py'}\")\n    \n    # Create scratch and results directories\n    SCRATCH_DIR.mkdir(parents=True, exist_ok=True)\n    RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"Environment setup complete!\")\n    print(\"=\" * 60)\n    return True\n\n\ndef cleanup_scratch():\n    \"\"\"Remove the scratch directory.\"\"\"\n    if SCRATCH_DIR.exists():\n        shutil.rmtree(SCRATCH_DIR)\n        print(f\"Cleaned up: {SCRATCH_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Initialize Environment\n",
    "\n",
    "Run this cell to setup the environment. This will:\n",
    "- Load required modules\n",
    "- Create necessary directories\n",
    "- Sync uv dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 3.4 System Information\n\nDynamically collect system specifications from the current node.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import socket\nimport platform\nimport re\n\ndef get_system_info() -> dict:\n    \"\"\"\n    Collect system information dynamically.\n    \n    Returns\n    -------\n    dict\n        Dictionary with system specifications\n    \"\"\"\n    info = {\n        \"hostname\": socket.gethostname(),\n        \"platform\": platform.platform(),\n        \"processor\": platform.processor(),\n    }\n    \n    # CPU info\n    try:\n        with open(\"/proc/cpuinfo\", \"r\") as f:\n            cpuinfo = f.read()\n        \n        # Count physical cores\n        physical_ids = set(re.findall(r\"physical id\\s*:\\s*(\\d+)\", cpuinfo))\n        cores_per_socket = len(set(re.findall(r\"core id\\s*:\\s*(\\d+)\", cpuinfo)))\n        total_cores = len(re.findall(r\"^processor\\s*:\", cpuinfo, re.MULTILINE))\n        \n        # Get CPU model name\n        model_match = re.search(r\"model name\\s*:\\s*(.+)\", cpuinfo)\n        if model_match:\n            info[\"cpu_model\"] = model_match.group(1).strip()\n        else:\n            # For ARM, try to get CPU part\n            cpu_part = re.search(r\"CPU part\\s*:\\s*(.+)\", cpuinfo)\n            info[\"cpu_model\"] = f\"ARM {cpu_part.group(1).strip()}\" if cpu_part else \"Unknown\"\n        \n        info[\"cpu_sockets\"] = len(physical_ids) if physical_ids else 1\n        info[\"cpu_cores_total\"] = total_cores\n    except Exception as e:\n        info[\"cpu_error\"] = str(e)\n    \n    # Memory info\n    try:\n        with open(\"/proc/meminfo\", \"r\") as f:\n            meminfo = f.read()\n        \n        mem_match = re.search(r\"MemTotal:\\s*(\\d+)\\s*kB\", meminfo)\n        if mem_match:\n            mem_kb = int(mem_match.group(1))\n            info[\"memory_gb\"] = round(mem_kb / 1024 / 1024, 1)\n    except Exception as e:\n        info[\"memory_error\"] = str(e)\n    \n    # GPU info (nvidia-smi)\n    try:\n        result = subprocess.run(\n            [\"nvidia-smi\", \"--query-gpu=name,memory.total,count\", \"--format=csv,noheader,nounits\"],\n            capture_output=True, text=True, timeout=10\n        )\n        if result.returncode == 0:\n            gpu_lines = result.stdout.strip().split(\"\\n\")\n            gpu_count = len(gpu_lines)\n            if gpu_lines and gpu_lines[0]:\n                parts = gpu_lines[0].split(\", \")\n                info[\"gpu_model\"] = parts[0].strip()\n                info[\"gpu_memory_mb\"] = int(parts[1].strip()) if len(parts) > 1 else 0\n                info[\"gpu_count\"] = gpu_count\n    except Exception as e:\n        info[\"gpu_info\"] = \"Not available\"\n    \n    # Storage info\n    try:\n        result = subprocess.run(\n            [\"df\", \"-h\", \"--output=size,avail,target\", \"/\"],\n            capture_output=True, text=True, timeout=10\n        )\n        if result.returncode == 0:\n            lines = result.stdout.strip().split(\"\\n\")\n            if len(lines) > 1:\n                parts = lines[1].split()\n                info[\"storage_total\"] = parts[0]\n                info[\"storage_available\"] = parts[1]\n    except Exception as e:\n        info[\"storage_error\"] = str(e)\n    \n    return info\n\n\ndef display_system_info(info: dict):\n    \"\"\"Display system information as a formatted table.\"\"\"\n    print(\"=\" * 60)\n    print(\"SYSTEM INFORMATION\")\n    print(\"=\" * 60)\n    print(f\"Hostname: {info.get('hostname', 'Unknown')}\")\n    print(f\"Platform: {info.get('platform', 'Unknown')}\")\n    print()\n    \n    # CPU\n    print(\"CPU:\")\n    print(f\"  Model: {info.get('cpu_model', 'Unknown')}\")\n    print(f\"  Sockets: {info.get('cpu_sockets', 'Unknown')}\")\n    print(f\"  Total Cores: {info.get('cpu_cores_total', 'Unknown')}\")\n    print()\n    \n    # Memory\n    print(\"Memory:\")\n    print(f\"  Total: {info.get('memory_gb', 'Unknown')} GiB\")\n    print()\n    \n    # GPU\n    if \"gpu_model\" in info:\n        print(\"GPU:\")\n        print(f\"  Model: {info.get('gpu_model', 'Unknown')}\")\n        print(f\"  Count: {info.get('gpu_count', 'Unknown')}\")\n        print(f\"  Memory per GPU: {info.get('gpu_memory_mb', 0) / 1024:.0f} GB\")\n    else:\n        print(\"GPU: Not available\")\n    print()\n    \n    # Storage\n    print(\"Storage:\")\n    print(f\"  Total: {info.get('storage_total', 'Unknown')}\")\n    print(f\"  Available: {info.get('storage_available', 'Unknown')}\")\n    print(\"=\" * 60)\n    \n    return info\n\n\n# Collect and display system info\nsystem_info = get_system_info()\ndisplay_system_info(system_info)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create a summary table for the report\ndef system_info_table(info: dict) -> pd.DataFrame:\n    \"\"\"Create a DataFrame with system specifications.\"\"\"\n    \n    gpu_spec = \"Not available\"\n    if \"gpu_model\" in info:\n        gpu_mem_gb = info.get('gpu_memory_mb', 0) / 1024\n        gpu_spec = f\"{info.get('gpu_count', 1)}x {info.get('gpu_model', 'Unknown')} ({gpu_mem_gb:.0f}GB each)\"\n    \n    data = [\n        (\"CPU\", f\"{info.get('cpu_sockets', 1)}x {info.get('cpu_model', 'Unknown')} ({info.get('cpu_cores_total', 'Unknown')} cores total)\"),\n        (\"Memory (RAM)\", f\"{info.get('memory_gb', 'Unknown')} GiB\"),\n        (\"GPU\", gpu_spec),\n        (\"Storage\", f\"{info.get('storage_total', 'Unknown')} (Available: {info.get('storage_available', 'Unknown')})\"),\n    ]\n    \n    return pd.DataFrame(data, columns=[\"Component\", \"Specification\"])\n\n\n# Display as table\nsystem_table = system_info_table(system_info)\ndisplay(system_table.style.hide(axis='index').set_properties(**{'text-align': 'left'}))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimental Design\n",
    "\n",
    "### 4.1 Load Experiment Configuration\n",
    "\n",
    "The experimental design defines all benchmark runs to execute. The `run` column indicates completion status:\n",
    "- `N` = Not yet run (pending)\n",
    "- `Y` = Completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_experiment_design() -> pd.DataFrame:\n    \"\"\"Load the experimental design CSV, skipping comment lines.\"\"\"\n    df = pd.read_csv(EXPERIMENT_FILE, comment='#')\n    # Fill NaN in overrides with empty string\n    if 'overrides' in df.columns:\n        df['overrides'] = df['overrides'].fillna('')\n    return df\n\n\ndef save_experiment_design(df: pd.DataFrame):\n    \"\"\"Save the experimental design CSV.\"\"\"\n    df.to_csv(EXPERIMENT_FILE, index=False)\n    print(f\"Saved experiment design to {EXPERIMENT_FILE}\")\n\n\ndef get_pending_experiments(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Get experiments that haven't been run yet.\"\"\"\n    return df[df['run'] == 'N'].copy()\n\n\ndef get_experiment_id(row: pd.Series) -> str:\n    \"\"\"Generate a unique ID for an experiment based on its parameters.\"\"\"\n    overrides = row.get('overrides', '')\n    if overrides:\n        # Extract parameter name and value for folder name\n        # e.g., \"++workload.reader.batch_size=1\" -> \"batch_size_1\"\n        param = overrides.split('=')[0].split('.')[-1]\n        value = overrides.split('=')[1] if '=' in overrides else ''\n        return f\"{row['processes']}_{param}_{value}\"\n    return str(row['processes'])\n\n\ndef mark_experiment_complete(df: pd.DataFrame, model: str, processes: int, overrides: str = '') -> pd.DataFrame:\n    \"\"\"Mark an experiment as completed.\"\"\"\n    mask = (df['model'] == model) & (df['processes'] == processes)\n    if 'overrides' in df.columns:\n        mask = mask & (df['overrides'] == overrides)\n    df.loc[mask, 'run'] = 'Y'\n    return df\n\n\n# Load experiment design\nexperiment_df = load_experiment_design()\n\nprint(\"=\" * 60)\nprint(\"EXPERIMENTAL DESIGN\")\nprint(\"=\" * 60)\nprint(f\"Total experiments: {len(experiment_df)}\")\nprint(f\"Completed: {(experiment_df['run'] == 'Y').sum()}\")\nprint(f\"Pending: {(experiment_df['run'] == 'N').sum()}\")\nprint(\"\\nExperiment matrix:\")\nexperiment_df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_experiment_status(df: pd.DataFrame):\n    \"\"\"\n    Visualize experiment completion status.\n    \"\"\"\n    # Create a summary by parameter category\n    df = df.copy()\n    df['category'] = df['overrides'].apply(\n        lambda x: x.split('.')[-1].split('=')[0] if x else 'baseline'\n    )\n    \n    # Count by category and status\n    summary = df.groupby(['category', 'run']).size().unstack(fill_value=0)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Pie chart of overall status\n    status_counts = df['run'].value_counts()\n    colors = ['#ccffcc' if s == 'Y' else '#ffcccc' for s in status_counts.index]\n    axes[0].pie(status_counts, labels=['Completed' if s == 'Y' else 'Pending' for s in status_counts.index],\n                colors=colors, autopct='%1.1f%%', startangle=90)\n    axes[0].set_title('Overall Experiment Status', fontsize=14, fontweight='bold')\n    \n    # Bar chart by category\n    if 'Y' not in summary.columns:\n        summary['Y'] = 0\n    if 'N' not in summary.columns:\n        summary['N'] = 0\n    \n    x = range(len(summary))\n    width = 0.35\n    axes[1].bar([i - width/2 for i in x], summary['N'], width, label='Pending', color='#ffcccc')\n    axes[1].bar([i + width/2 for i in x], summary['Y'], width, label='Completed', color='#ccffcc')\n    axes[1].set_xticks(x)\n    axes[1].set_xticklabels(summary.index, rotation=45, ha='right')\n    axes[1].set_ylabel('Number of Experiments')\n    axes[1].set_title('Experiments by Parameter Category', fontsize=14, fontweight='bold')\n    axes[1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n\n\nplot_experiment_status(experiment_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmark Execution\n",
    "\n",
    "### 5.1 Benchmark Runner\n",
    "\n",
    "Functions to execute DLIO benchmarks based on the experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_data(model: str, num_procs: int = 8, overrides: str = '') -> bool:\n    \"\"\"\n    Generate synthetic data for a workload.\n    \n    Parameters\n    ----------\n    model : str\n        Workload name (e.g., 'unet3d_h100_custom')\n    num_procs : int\n        Number of MPI processes for data generation\n    overrides : str\n        Hydra overrides (e.g., '++workload.reader.batch_size=1')\n        \n    Returns\n    -------\n    bool\n        True if successful\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"GENERATING DATA: {model}\")\n    if overrides:\n        print(f\"Overrides: {overrides}\")\n    print(f\"{'='*60}\")\n    \n    cmd = f\"\"\"\n    mpirun -np {num_procs} \\\n        uv run --project {BASE_DIR} dlio_benchmark \\\n        --config-dir {CONFIG_DIR} \\\n        workload={model} \\\n        ++workload.workflow.generate_data=True \\\n        ++workload.workflow.train=False \\\n        ++workload.workflow.evaluation=False \\\n        {overrides}\n    \"\"\".strip()\n    \n    ret, _, _ = run_command(cmd, cwd=SCRATCH_DIR)\n    return ret == 0\n\n\ndef run_benchmark(model: str, num_procs: int, overrides: str = '', experiment_id: str = None) -> bool:\n    \"\"\"\n    Run a benchmark for a specific model and process count.\n    \n    Parameters\n    ----------\n    model : str\n        Workload name\n    num_procs : int\n        Number of MPI processes\n    overrides : str\n        Hydra overrides (e.g., '++workload.reader.batch_size=1')\n    experiment_id : str\n        Unique ID for this experiment (used for results folder)\n        \n    Returns\n    -------\n    bool\n        True if successful\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"RUNNING BENCHMARK: {model} with {num_procs} processes\")\n    if overrides:\n        print(f\"Overrides: {overrides}\")\n    print(f\"{'='*60}\")\n    \n    cmd = f\"\"\"\n    mpirun -np {num_procs} \\\n        uv run --project {BASE_DIR} dlio_benchmark \\\n        --config-dir {CONFIG_DIR} \\\n        workload={model} \\\n        ++workload.workflow.generate_data=False \\\n        ++workload.workflow.train=True \\\n        ++workload.workflow.evaluation=True \\\n        {overrides}\n    \"\"\".strip()\n    \n    ret, _, _ = run_command(cmd, cwd=SCRATCH_DIR)\n    \n    if ret == 0:\n        # Copy results - use experiment_id for unique folder name\n        result_src = SCRATCH_DIR / \"hydra_log\" / model\n        folder_name = experiment_id if experiment_id else str(num_procs)\n        result_dst = RESULTS_DIR / model / folder_name\n        \n        if result_src.exists():\n            # Use copytree to handle directories recursively\n            if result_dst.exists():\n                shutil.rmtree(result_dst)\n            shutil.copytree(result_src, result_dst)\n            print(f\"\\nResults copied to: {result_dst}\")\n            \n            # Clean up hydra_log\n            shutil.rmtree(result_src)\n    \n    return ret == 0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Run Single Experiment\n",
    "\n",
    "Use this cell to run a single experiment. Modify the parameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SINGLE EXPERIMENT - Modify these parameters\n# ============================================================\nRUN_MODEL = \"unet3d_h100_custom\"  # Model to run\nRUN_PROCS = 4                      # Number of processes\nRUN_OVERRIDES = \"\"                 # Hydra overrides (e.g., \"++workload.reader.batch_size=1\")\nGENERATE_DATA_FIRST = True         # Generate data before benchmark?\n# ============================================================\n\n# Check if already completed\nexp_row = experiment_df[\n    (experiment_df['model'] == RUN_MODEL) & \n    (experiment_df['processes'] == RUN_PROCS) &\n    (experiment_df['overrides'] == RUN_OVERRIDES)\n]\n\nif not exp_row.empty and exp_row.iloc[0]['run'] == 'Y':\n    print(f\"Experiment {RUN_MODEL} with {RUN_PROCS} procs already completed.\")\n    print(\"Set run='N' in the CSV to re-run, or modify parameters above.\")\nelse:\n    print(f\"Will run: {RUN_MODEL} with {RUN_PROCS} processes\")\n    if RUN_OVERRIDES:\n        print(f\"Overrides: {RUN_OVERRIDES}\")\n    print(f\"Generate data first: {GENERATE_DATA_FIRST}\")\n    print(\"\\nExecute the next cell to start the benchmark.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Execute the single experiment\nexperiment_id = get_experiment_id(pd.Series({\n    'processes': RUN_PROCS,\n    'overrides': RUN_OVERRIDES\n}))\n\nif GENERATE_DATA_FIRST:\n    if not generate_data(RUN_MODEL, RUN_PROCS, RUN_OVERRIDES):\n        raise RuntimeError(f\"Data generation failed for {RUN_MODEL}\")\n\nif run_benchmark(RUN_MODEL, RUN_PROCS, RUN_OVERRIDES, experiment_id):\n    print(f\"\\n{'='*60}\")\n    print(\"SUCCESS!\")\n    print(f\"{'='*60}\")\n    \n    # Update experiment status\n    experiment_df = mark_experiment_complete(experiment_df, RUN_MODEL, RUN_PROCS, RUN_OVERRIDES)\n    save_experiment_design(experiment_df)\n    \n    print(f\"\\nUpdated experiment status for {RUN_MODEL} ({experiment_id})\")\nelse:\n    print(f\"\\n{'='*60}\")\n    print(\"FAILED!\")\n    print(f\"{'='*60}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Run All Pending Experiments\n",
    "\n",
    "This will run all experiments marked as `N` (pending) in the experimental design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_all_pending_experiments(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Run all pending experiments from the experimental design.\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Experimental design DataFrame\n        \n    Returns\n    -------\n    pd.DataFrame\n        Updated DataFrame with completion status\n    \"\"\"\n    pending = get_pending_experiments(df)\n    \n    if pending.empty:\n        print(\"No pending experiments to run!\")\n        return df\n    \n    print(f\"Found {len(pending)} pending experiments\")\n    print(\"\\nPending experiments:\")\n    display(pending)\n    \n    # Group by model to potentially reuse generated data\n    models = pending['model'].unique()\n    \n    for model in models:\n        print(f\"\\n{'#'*60}\")\n        print(f\"# Processing model: {model}\")\n        print(f\"{'#'*60}\")\n        \n        model_pending = pending[pending['model'] == model]\n        \n        # Generate data once for baseline (no overrides that affect data)\n        # Data-affecting overrides: format, record_length_bytes\n        baseline_pending = model_pending[~model_pending['overrides'].str.contains('format|record_length', na=False)]\n        if not baseline_pending.empty:\n            max_procs = baseline_pending['processes'].max()\n            if not generate_data(model, num_procs=max_procs):\n                print(f\"ERROR: Data generation failed for {model}\")\n                continue\n        \n        # Run benchmarks for each experiment\n        for idx, row in model_pending.iterrows():\n            procs = row['processes']\n            overrides = row.get('overrides', '')\n            experiment_id = get_experiment_id(row)\n            \n            # If this experiment changes data format/size, regenerate data\n            if 'format' in overrides or 'record_length' in overrides:\n                if not generate_data(model, num_procs=procs, overrides=overrides):\n                    print(f\"ERROR: Data generation failed for {model} with {overrides}\")\n                    continue\n            \n            if run_benchmark(model, procs, overrides, experiment_id):\n                df = mark_experiment_complete(df, model, procs, overrides)\n                save_experiment_design(df)\n                print(f\"Marked {model} ({experiment_id}) as complete\")\n            else:\n                print(f\"ERROR: Benchmark failed for {model} ({experiment_id})\")\n    \n    return df\n\n\n# Show pending experiments\npending = get_pending_experiments(experiment_df)\nprint(f\"Pending experiments: {len(pending)}\")\nif not pending.empty:\n    display(pending)\n    print(\"\\nExecute the next cell to run all pending experiments.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all pending experiments\n",
    "# WARNING: This may take a long time!\n",
    "\n",
    "experiment_df = run_all_pending_experiments(experiment_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "plot_experiment_status(experiment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis\n",
    "\n",
    "### 6.1 Load Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_benchmark_results(results_dir: Path) -> pd.DataFrame:\n    \"\"\"\n    Load all benchmark summary.json files.\n    Filters out failed runs where both AU and I/O are zero.\n    \"\"\"\n    data = []\n    \n    for file_path in glob.glob(str(results_dir / \"**/summary.json\"), recursive=True):\n        with open(file_path, \"r\") as f:\n            summary = json.load(f)\n        \n        path_parts = Path(file_path).parts\n        try:\n            results_idx = path_parts.index('results')\n            model_name = path_parts[results_idx + 1]\n            experiment_id = path_parts[results_idx + 2]  # e.g., \"4\" or \"4_batch_size_1\"\n        except (ValueError, IndexError):\n            model_name = Path(file_path).parent.parent.name\n            experiment_id = Path(file_path).parent.name\n        \n        # Parse experiment_id to extract processes and parameter\n        parts = experiment_id.split('_')\n        try:\n            processes = int(parts[0])\n        except ValueError:\n            processes = 0\n        \n        # Extract parameter info if present\n        if len(parts) > 1:\n            param_name = parts[1]\n            param_value = '_'.join(parts[2:]) if len(parts) > 2 else ''\n        else:\n            param_name = 'baseline'\n            param_value = ''\n        \n        num_accelerators = summary.get(\"num_accelerators\", 0)\n        metrics = summary.get(\"metric\", {})\n        \n        data.append({\n            'model': model_name,\n            'experiment_id': experiment_id,\n            'processes': processes,\n            'parameter': param_name,\n            'value': param_value,\n            'accelerator_usage': metrics.get(\"train_au_mean_percentage\", 0),\n            'accelerator_usage_std': metrics.get(\"train_au_stdev_percentage\", 0),\n            'io_throughput': metrics.get(\"train_io_mean_MB_per_second\", 0),\n            'io_throughput_std': metrics.get(\"train_io_stdev_MB_per_second\", 0)\n        })\n    \n    df = pd.DataFrame(data)\n    if not df.empty:\n        # Filter out failed runs where both AU and I/O are zero\n        before_count = len(df)\n        df = df[~((df['accelerator_usage'] == 0) & (df['io_throughput'] == 0))]\n        filtered_count = before_count - len(df)\n        if filtered_count > 0:\n            print(f\"Filtered out {filtered_count} failed runs (AU=0 and I/O=0)\")\n        \n        # Sort results\n        df = df.sort_values(by=[\"model\", \"parameter\", \"processes\"]).reset_index(drop=True)\n    \n    return df\n\n\n# Load results\nresults_df = load_benchmark_results(RESULTS_DIR)\nprint(f\"Loaded {len(results_df)} valid benchmark results\")\nresults_df"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Accelerator Usage vs. Number of Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_accelerator_usage(df: pd.DataFrame):\n    \"\"\"Plot accelerator usage grouped by parameter.\"\"\"\n    if df.empty:\n        print(\"No data to plot\")\n        return\n    \n    # Get unique parameters\n    parameters = df['parameter'].unique()\n    n_params = len(parameters)\n    \n    fig, axes = plt.subplots(1, min(n_params, 3), figsize=(6 * min(n_params, 3), 6), squeeze=False)\n    axes = axes.flatten()\n    \n    for idx, param in enumerate(parameters[:3]):  # Show first 3 parameters\n        ax = axes[idx]\n        param_df = df[df['parameter'] == param]\n        \n        if param == 'baseline':\n            # For baseline, plot by processes\n            param_df = param_df.sort_values('processes')\n            ax.errorbar(\n                param_df['processes'], param_df['accelerator_usage'],\n                yerr=param_df['accelerator_usage_std'],\n                marker='o', markersize=8, linewidth=2, capsize=5\n            )\n            ax.set_xlabel('Number of Processes')\n        else:\n            # For parameters, plot by value\n            ax.bar(param_df['value'].astype(str), param_df['accelerator_usage'],\n                   yerr=param_df['accelerator_usage_std'], capsize=5)\n            ax.set_xlabel(param)\n            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n        \n        ax.set_ylabel('Accelerator Usage (%)')\n        ax.set_title(f'AU: {param}', fontweight='bold')\n        ax.grid(True, alpha=0.3)\n        ax.set_ylim(0, 100)\n    \n    plt.tight_layout()\n    plt.savefig(\"accelerator_usage.png\", dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # If more than 3 parameters, show remaining\n    if n_params > 3:\n        remaining = parameters[3:]\n        fig2, axes2 = plt.subplots(1, len(remaining), figsize=(6 * len(remaining), 6), squeeze=False)\n        axes2 = axes2.flatten()\n        for idx, param in enumerate(remaining):\n            ax = axes2[idx]\n            param_df = df[df['parameter'] == param]\n            ax.bar(param_df['value'].astype(str), param_df['accelerator_usage'],\n                   yerr=param_df['accelerator_usage_std'], capsize=5)\n            ax.set_xlabel(param)\n            ax.set_ylabel('Accelerator Usage (%)')\n            ax.set_title(f'AU: {param}', fontweight='bold')\n            ax.grid(True, alpha=0.3)\n            ax.set_ylim(0, 100)\n            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n        plt.tight_layout()\n        plt.show()\n\n\nplot_accelerator_usage(results_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 I/O Throughput vs. Number of Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_io_throughput(df: pd.DataFrame):\n    \"\"\"Plot I/O throughput grouped by parameter.\"\"\"\n    if df.empty:\n        print(\"No data to plot\")\n        return\n    \n    # Get unique parameters\n    parameters = df['parameter'].unique()\n    n_params = len(parameters)\n    \n    fig, axes = plt.subplots(1, min(n_params, 3), figsize=(6 * min(n_params, 3), 6), squeeze=False)\n    axes = axes.flatten()\n    \n    for idx, param in enumerate(parameters[:3]):\n        ax = axes[idx]\n        param_df = df[df['parameter'] == param]\n        \n        if param == 'baseline':\n            param_df = param_df.sort_values('processes')\n            ax.errorbar(\n                param_df['processes'], param_df['io_throughput'],\n                yerr=param_df['io_throughput_std'],\n                marker='s', markersize=8, linewidth=2, capsize=5, color='#2ecc71'\n            )\n            ax.set_xlabel('Number of Processes')\n        else:\n            ax.bar(param_df['value'].astype(str), param_df['io_throughput'],\n                   yerr=param_df['io_throughput_std'], capsize=5, color='#2ecc71')\n            ax.set_xlabel(param)\n            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n        \n        ax.set_ylabel('I/O Throughput (MB/s)')\n        ax.set_title(f'I/O: {param}', fontweight='bold')\n        ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(\"io_throughput.png\", dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # If more than 3 parameters, show remaining\n    if n_params > 3:\n        remaining = parameters[3:]\n        fig2, axes2 = plt.subplots(1, len(remaining), figsize=(6 * len(remaining), 6), squeeze=False)\n        axes2 = axes2.flatten()\n        for idx, param in enumerate(remaining):\n            ax = axes2[idx]\n            param_df = df[df['parameter'] == param]\n            ax.bar(param_df['value'].astype(str), param_df['io_throughput'],\n                   yerr=param_df['io_throughput_std'], capsize=5, color='#2ecc71')\n            ax.set_xlabel(param)\n            ax.set_ylabel('I/O Throughput (MB/s)')\n            ax.set_title(f'I/O: {param}', fontweight='bold')\n            ax.grid(True, alpha=0.3)\n            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n        plt.tight_layout()\n        plt.show()\n\n\nplot_io_throughput(results_df)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not results_df.empty:\n    summary = results_df.copy()\n    summary['accelerator_usage'] = summary['accelerator_usage'].round(2)\n    summary['io_throughput'] = summary['io_throughput'].round(2)\n    \n    # Select and rename columns for display\n    display_cols = ['model', 'parameter', 'value', 'processes', 'accelerator_usage', 'io_throughput']\n    summary = summary[display_cols]\n    summary.columns = ['Model', 'Parameter', 'Value', 'Procs', 'AU (%)', 'I/O (MB/s)']\n    \n    display(summary.style.background_gradient(subset=['AU (%)'], cmap='RdYlGn', vmin=0, vmax=100)\n                        .background_gradient(subset=['I/O (MB/s)'], cmap='Blues'))\n    \n    # Save results to CSV\n    output_csv = RESULTS_DIR / \"benchmark_results.csv\"\n    summary.to_csv(output_csv, index=False)\n    print(f\"\\nResults saved to: {output_csv}\")\nelse:\n    print(\"No results to display\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup\n",
    "\n",
    "Run this cell to clean up the scratch directory when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_scratch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on the analysis of the benchmark results:\n",
    "\n",
    "1. **Accelerator Usage Patterns:** [Add observations]\n",
    "\n",
    "2. **I/O Throughput Scaling:** [Add observations]\n",
    "\n",
    "3. **Model-Specific Characteristics:** [Add observations]\n",
    "\n",
    "---\n",
    "\n",
    "**Repository:** https://github.com/HpcResearchLaboratory/perf_2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}